{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChapelFob80930/AI-Generated-Text-Detector/blob/main/AITextTest2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DivI8E17Ug5",
        "outputId": "ee882b61-988a-485d-b90b-bb4d23d9ce05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   prompt_id                       prompt_name  \\\n",
            "0          0                   Car-free cities   \n",
            "1          1  Does the electoral college work?   \n",
            "\n",
            "                                        instructions  \\\n",
            "0  Write an explanatory essay to inform fellow ci...   \n",
            "1  Write a letter to your state senator in which ...   \n",
            "\n",
            "                                         source_text  \n",
            "0  # In German Suburb, Life Goes On Without Cars ...  \n",
            "1  # What Is the Electoral College? by the Office...  \n",
            "\n",
            "\n",
            "         id  prompt_id                                               text  \\\n",
            "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
            "1  005db917          0  Transportation is a large necessity in most co...   \n",
            "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
            "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
            "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
            "\n",
            "   generated  \n",
            "0          0  \n",
            "1          0  \n",
            "2          0  \n",
            "3          0  \n",
            "4          0  \n",
            "\n",
            "\n",
            "         id  prompt_id          text\n",
            "0  0000aaaa          2  Aaa bbb ccc.\n",
            "1  1111bbbb          3  Bbb ccc ddd.\n",
            "2  2222cccc          4  CCC ddd eee.\n",
            "                                                text  generated\n",
            "0  Cars. Cars have been around since they became ...        0.0\n",
            "1  Transportation is a large necessity in most co...        0.0\n",
            "2  \"America's love affair with it's vehicles seem...        0.0\n",
            "3  How often do you ride in a car? Do you drive a...        0.0\n",
            "4  Cars are a wonderful thing. They are perhaps o...        0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train_prompts = pd.read_csv(\"https://raw.githubusercontent.com/ChapelFob80930/AI-Generated-Text-Detector/refs/heads/main/train_prompts.csv\")\n",
        "print(train_prompts.head())\n",
        "print(\"\\n\")\n",
        "train_essays = pd.read_csv(\"https://raw.githubusercontent.com/ChapelFob80930/AI-Generated-Text-Detector/refs/heads/main/train_essays.csv\")\n",
        "print(train_essays.head())\n",
        "print(\"\\n\")\n",
        "test_essays = pd.read_csv(\"https://raw.githubusercontent.com/ChapelFob80930/AI-Generated-Text-Detector/refs/heads/main/test_essays.csv\")\n",
        "print(test_essays.head())\n",
        "ai_human_dataset = pd.read_csv(\"/content/AI_Human.csv\")\n",
        "print(ai_human_dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gsRlIA2q7kps",
        "outputId": "8f3a2b44-5743-4083-c22f-46ba2699986b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_eng to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_rus to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_rus.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_ne_chunker_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package punkt_tab to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt_tab.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package tagsets_json to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets_json.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  doc = nlp(\" \".join(text))\n",
        "  return [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "KtQ0Ruc1qZGW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def _remove_stopwords_(texts):\n",
        "  return [text for text in texts if text.isalnum() and text not in stop_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWbPW99ZqEQi",
        "outputId": "73ad8ff5-0e62-4e5e-ee38-9e343aabc3f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4HoVvtL8tfB",
        "outputId": "b3a0dcd5-7b4f-4e9f-ed51-9e8ba26b5b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(series):\n",
        "    def process(text):\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = _remove_stopwords_(tokens)\n",
        "        tokens = lemmatize_text(tokens)\n",
        "        return \" \".join(tokens)\n",
        "    return series.apply(process)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_essays[\"preprocessedText\"]=preprocess_text(train_essays['text'])"
      ],
      "metadata": {
        "id": "goB1poMYtSK_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_human_dataset[\"preprocessedText\"]=preprocess_text(ai_human_dataset['text'])"
      ],
      "metadata": {
        "id": "9lM7gsVmrcue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_essays.head())"
      ],
      "metadata": {
        "id": "oAyccXk8xdTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_human_dataset.head())"
      ],
      "metadata": {
        "id": "3hBGujKSrl7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# sentences = train_essays['preprocessedText'].apply(lambda x: x.split())\n",
        "\n",
        "# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# word_vector = word2vec_model.wv['car']\n",
        "# print(\"Vector for 'car':\", word_vector)\n",
        "\n",
        "# # train_essays['word2vecVector'] = sentences.apply(\n",
        "# #     lambda x: sum([word2vec_model.wv[word] for word in x if word in word2vec_model.wv]) / len(x)\n",
        "# # )\n",
        "\n",
        "# def calculate_average_word_vector(words,model,vector_size=100):\n",
        "#     word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "#     if word_vectors:\n",
        "#         return np.mean(word_vectors, axis=0)\n",
        "#     else:\n",
        "#         return np.zeros(vector_size)\n",
        "\n",
        "# train_essays['word2vecVector'] = train_essays['preprocessedText'].apply(lambda x: calculate_average_word_vector(x.split(),word2vec_model))\n"
      ],
      "metadata": {
        "id": "TIsyFKawzzdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = ai_human_dataset[\"preprocessedText\"].apply(lambda x: x.split())\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "word_vector = word2vec_model.wv['car']\n",
        "print(\"Vector for 'car':\", word_vector)\n",
        "\n",
        "# train_essays['word2vecVector'] = sentences.apply(\n",
        "#     lambda x: sum([word2vec_model.wv[word] for word in x if word in word2vec_model.wv]) / len(x)\n",
        "# )\n",
        "\n",
        "def calculate_average_word_vector(words,model,vector_size=100):\n",
        "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "ai_human_dataset['word2vecVector'] = ai_human_dataset[\"preprocessedText\"].apply(lambda x: calculate_average_word_vector(x.split(),word2vec_model))\n"
      ],
      "metadata": {
        "id": "9W9VYPw_z3Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(train_essays['preprocessedText'])]\n",
        "\n",
        "# doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# train_essays['doc2vecVector'] = train_essays.index.map(lambda x: doc2vec_model.dv[str(x)])\n"
      ],
      "metadata": {
        "id": "zKrP9kmsgqv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(ai_human_dataset[\"preprocessedText\"])]\n",
        "\n",
        "doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "ai_human_dataset['doc2vecVector'] = ai_human_dataset.index.map(lambda x: doc2vec_model.dv[str(x)])"
      ],
      "metadata": {
        "id": "VDkQdUMS0GRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_essays.head())"
      ],
      "metadata": {
        "id": "JeU2USKZg8LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# y = train_essays['generated']\n",
        "\n",
        "# train_index, test_index = train_test_split(train_essays.index, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "FsW-bZJVhUXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = ai_human_dataset['generated']\n",
        "\n",
        "train_index, test_index = train_test_split(ai_human_dataset.index, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_i0ssR6L0dp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "# X_tfidf = tfidf_vectorizer.fit_transform(train_essays['preprocessedText'])\n",
        "\n",
        "# X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
        "# y_train, y_test = y[train_index], y[test_index]\n"
      ],
      "metadata": {
        "id": "lSBF5CjxhlFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(ai_human_dataset[\"preprocessedText\"])\n",
        "\n",
        "X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
        "y_train, y_test = y[train_index], y[test_index]\n"
      ],
      "metadata": {
        "id": "PH4Lysdq0osa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_human_dataset['word2vecVector'] = ai_human_dataset['word2vecVector'].apply(lambda x: np.array(x))\n",
        "\n",
        "X_word2vec = np.stack(ai_human_dataset['word2vecVector'].values)\n",
        "\n",
        "X_train_word2vec, X_test_word2vec = X_word2vec[train_index], X_word2vec[test_index]\n",
        "\n",
        "print(X_train_word2vec.shape)\n",
        "print(X_test_word2vec.shape)\n",
        "print(X_train_word2vec)"
      ],
      "metadata": {
        "id": "JLjvTwhYjg5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_essays['word2vecVector'] = train_essays['word2vecVector'].apply(lambda x: np.array(x))\n",
        "\n",
        "# X_word2vec = np.stack(train_essays['word2vecVector'].values)\n",
        "\n",
        "# X_train_word2vec, X_test_word2vec = X_word2vec[train_index], X_word2vec[test_index]\n",
        "\n",
        "# print(X_train_word2vec.shape)\n",
        "# print(X_test_word2vec.shape)\n",
        "# print(X_train_word2vec)"
      ],
      "metadata": {
        "id": "y_WIf5vY0_4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_human_dataset['doc2vecVector'] = ai_human_dataset['doc2vecVector'].apply(lambda x: np.array(x))\n",
        "X_doc2vec = np.stack(ai_human_dataset['doc2vecVector'].values)\n",
        "X_train_doc2vec, X_test_doc2vec = X_doc2vec[train_index], X_doc2vec[test_index]\n",
        "print(X_train_doc2vec.shape)\n",
        "print(X_test_doc2vec.shape)\n",
        "print(X_train_doc2vec)"
      ],
      "metadata": {
        "id": "R_15evjXjwIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_essays['doc2vecVector'] = train_essays['doc2vecVector'].apply(lambda x: np.array(x))\n",
        "# X_doc2vec = np.stack(train_essays['doc2vecVector'].values)\n",
        "# X_train_doc2vec, X_test_doc2vec = X_doc2vec[train_index], X_doc2vec[test_index]\n",
        "# print(X_train_doc2vec.shape)\n",
        "# print(X_test_doc2vec.shape)\n",
        "# print(X_train_doc2vec)"
      ],
      "metadata": {
        "id": "dka8weX91R_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def evaluate_svm_model(X_train, X_test, y_train, y_test):\n",
        "    svm_model = SVC(class_weight='balanced')\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "\n",
        "results_tfidf = evaluate_svm_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results for SVM:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_svm_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results for SVM:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_svm_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results for SVM:\", results_doc2vec)\n"
      ],
      "metadata": {
        "id": "MeegEozDkbp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "def evaluate_decisionTree_model(X_train, X_test, y_train, y_test):\n",
        "    dt_model = DecisionTreeClassifier(random_state=42)\n",
        "    dt_model.fit(X_train, y_train)\n",
        "    y_pred = dt_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "results_tfidf = evaluate_decisionTree_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results for Decision Tree:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_decisionTree_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results for Decision Tree:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_decisionTree_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results for Decision Tree:\", results_doc2vec)\n"
      ],
      "metadata": {
        "id": "fwmYs9TX0uW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "def evaluate_randomForest_model(X_train, X_test, y_train, y_test):\n",
        "    rf_model = RandomForestClassifier(random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "results_tfidf = evaluate_randomForest_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results for Random Forest:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_randomForest_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results for Random Forest:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_randomForest_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results for Random Forest:\", results_doc2vec)"
      ],
      "metadata": {
        "id": "Ms6E55KI0wp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "def evaluate_knn_model(X_train, X_test, y_train, y_test):\n",
        "    knn_model = KNeighborsClassifier()\n",
        "    knn_model.fit(X_train, y_train)\n",
        "    y_pred = knn_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "results_tfidf = evaluate_knn_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results for KNN:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_knn_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results for KNN:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_knn_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results for KNN:\", results_doc2vec)\n",
        "\n",
        "#when u put in the correct dataset use error rate (1-accuracy) vs k values to get optimal k value at elbow point\n",
        ""
      ],
      "metadata": {
        "id": "J0yFlmdN0yuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q5m3E8tjqZ3p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFwYltgslGDCa1gM9QPAMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}