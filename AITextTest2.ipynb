{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChapelFob80930/AI-Generated-Text-Detector/blob/main/AITextTest2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DivI8E17Ug5",
        "outputId": "de1d7719-9167-4716-b679-832f5dabbae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   prompt_id                       prompt_name  \\\n",
            "0          0                   Car-free cities   \n",
            "1          1  Does the electoral college work?   \n",
            "\n",
            "                                        instructions  \\\n",
            "0  Write an explanatory essay to inform fellow ci...   \n",
            "1  Write a letter to your state senator in which ...   \n",
            "\n",
            "                                         source_text  \n",
            "0  # In German Suburb, Life Goes On Without Cars ...  \n",
            "1  # What Is the Electoral College? by the Office...  \n",
            "\n",
            "\n",
            "         id  prompt_id                                               text  \\\n",
            "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
            "1  005db917          0  Transportation is a large necessity in most co...   \n",
            "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
            "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
            "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
            "\n",
            "   generated  \n",
            "0          0  \n",
            "1          0  \n",
            "2          0  \n",
            "3          0  \n",
            "4          0  \n",
            "\n",
            "\n",
            "         id  prompt_id          text\n",
            "0  0000aaaa          2  Aaa bbb ccc.\n",
            "1  1111bbbb          3  Bbb ccc ddd.\n",
            "2  2222cccc          4  CCC ddd eee.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train_prompts = pd.read_csv(\"https://raw.githubusercontent.com/ChapelFob80930/AI-Generated-Text-Detector/refs/heads/main/train_prompts.csv\")\n",
        "print(train_prompts.head())\n",
        "print(\"\\n\")\n",
        "train_essays = pd.read_csv(\"https://raw.githubusercontent.com/ChapelFob80930/AI-Generated-Text-Detector/refs/heads/main/train_essays.csv\")\n",
        "print(train_essays.head())\n",
        "print(\"\\n\")\n",
        "test_essays = pd.read_csv(\"https://raw.githubusercontent.com/ChapelFob80930/AI-Generated-Text-Detector/refs/heads/main/test_essays.csv\")\n",
        "print(test_essays.head())\n",
        "# ai_human_dataset = pd.read_csv(\"/content/AI_Human.csv\")\n",
        "# print(ai_human_dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gsRlIA2q7kps",
        "outputId": "d945f4c9-5615-4ce1-882d-e9ff8366abc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_eng to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_rus to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_rus.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_ne_chunker_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package punkt_tab to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt_tab.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package tagsets_json to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets_json.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  doc = nlp(\" \".join(text))\n",
        "  return [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "KtQ0Ruc1qZGW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def _remove_stopwords_(texts):\n",
        "  return [text for text in texts if text.isalnum() and text not in stop_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWbPW99ZqEQi",
        "outputId": "827b9b5c-f832-4193-b38d-b629a9b41b0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4HoVvtL8tfB",
        "outputId": "239cca70-e55b-41b1-b2b4-60107d68f6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(series):\n",
        "    def process(text):\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = _remove_stopwords_(tokens)\n",
        "        tokens = lemmatize_text(tokens)\n",
        "        return \" \".join(tokens)\n",
        "    return series.apply(process)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_essays[\"preprocessedText\"]=preprocess_text(train_essays['text'])"
      ],
      "metadata": {
        "id": "goB1poMYtSK_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_essays.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAyccXk8xdTm",
        "outputId": "62e0224e-907c-4155-d812-be6f1293a3f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id  prompt_id                                               text  \\\n",
            "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
            "1  005db917          0  Transportation is a large necessity in most co...   \n",
            "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
            "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
            "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
            "\n",
            "   generated                                   preprocessedText  \n",
            "0          0  car Cars around since become famous 1900s Henr...  \n",
            "1          0  transportation large necessity country worldwi...  \n",
            "2          0  America love affair vehicle seem cool say Elis...  \n",
            "3          0  how often ride car do drive one motor vehicle ...  \n",
            "4          0  car wonderful thing they perhaps one world gre...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = train_essays['preprocessedText'].apply(lambda x: x.split())\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "word_vector = word2vec_model.wv['car']\n",
        "print(\"Vector for 'car':\", word_vector)\n",
        "\n",
        "# train_essays['word2vecVector'] = sentences.apply(\n",
        "#     lambda x: sum([word2vec_model.wv[word] for word in x if word in word2vec_model.wv]) / len(x)\n",
        "# )\n",
        "\n",
        "def calculate_average_word_vector(words,model,vector_size=100):\n",
        "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "train_essays['word2vecVector'] = train_essays['preprocessedText'].apply(lambda x: calculate_average_word_vector(x.split(),word2vec_model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIsyFKawzzdF",
        "outputId": "a24ba359-aa32-45ea-cfd3-29810a4ac75f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'car': [ 0.9798277  -0.21653482 -0.85568213  1.1064807  -0.44638282 -2.220694\n",
            "  3.2047741   0.26494384 -0.93324506  0.99873567  0.33852425  1.3901498\n",
            "  0.1737627   1.6806521  -0.5208178  -1.8023901   0.33743605 -1.1215509\n",
            " -0.9847663  -2.6631947   2.8432589   0.16027     0.44361705 -0.40433502\n",
            " -0.54924816 -1.4057215  -0.6584818  -0.13003407 -1.0804116  -0.883335\n",
            " -0.5462925   0.8330227   0.17081156  0.81720877  0.7923248  -0.48373425\n",
            "  0.38424653 -0.84581417  0.5594386  -2.2986102  -0.59632707  0.3130125\n",
            " -0.44109207  0.17779213 -0.25210264 -0.06692018 -0.10237622  0.84020907\n",
            "  2.529846    1.4646838   0.6271458   1.5595932   0.3636652   0.90081567\n",
            "  1.4002857  -1.190885    1.1704539  -0.8751411  -2.3014438   0.55963564\n",
            "  0.27354223 -0.23019417  0.03099069 -2.2606435  -0.75954056  0.9766482\n",
            "  0.19118032  0.45691997  0.01427387  0.17866276  0.5567877   0.09354579\n",
            " -0.27052915 -1.765663    1.5796745  -0.8201634   2.6097815   0.6628401\n",
            " -1.7131293  -0.3275655   0.1442206   0.96886194 -0.47690403  1.4076477\n",
            "  0.17657275 -1.0663668   1.0335114   0.00743648 -0.19553384  0.8949018\n",
            " -0.9116959   0.3281061   0.48557767 -0.6153311   2.261353    0.13577424\n",
            " -0.03885096 -0.6722943   0.19138841  0.15194972]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(train_essays['preprocessedText'])]\n",
        "\n",
        "doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "train_essays['doc2vecVector'] = train_essays.index.map(lambda x: doc2vec_model.dv[str(x)])\n"
      ],
      "metadata": {
        "id": "zKrP9kmsgqv_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_essays.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeU2USKZg8LN",
        "outputId": "259a28a1-98b6-4ac8-d1fd-b8d2a6991098"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id  prompt_id                                               text  \\\n",
            "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
            "1  005db917          0  Transportation is a large necessity in most co...   \n",
            "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
            "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
            "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
            "\n",
            "   generated                                   preprocessedText  \\\n",
            "0          0  car Cars around since become famous 1900s Henr...   \n",
            "1          0  transportation large necessity country worldwi...   \n",
            "2          0  America love affair vehicle seem cool say Elis...   \n",
            "3          0  how often ride car do drive one motor vehicle ...   \n",
            "4          0  car wonderful thing they perhaps one world gre...   \n",
            "\n",
            "                                      word2vecVector  \\\n",
            "0  [0.05613463, 0.01011937, -0.51041126, -0.31128...   \n",
            "1  [0.106938384, 0.034964364, -0.41481814, -0.186...   \n",
            "2  [-0.054207273, 0.08392249, -0.25987044, 0.0900...   \n",
            "3  [-0.033298444, 0.089855865, -0.38255462, -0.11...   \n",
            "4  [-0.084714, 0.18694308, -0.3413827, -0.0218881...   \n",
            "\n",
            "                                       doc2vecVector  \n",
            "0  [-0.313461, -0.19714004, -0.4053769, -0.681860...  \n",
            "1  [-0.22592835, 0.15463687, -0.29206222, -0.1083...  \n",
            "2  [-0.45858562, -0.31703842, -0.06265149, -0.330...  \n",
            "3  [-0.2774684, 0.10494252, -0.6182319, -0.185383...  \n",
            "4  [-0.5117222, 0.112322584, -0.78852177, -0.0566...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = train_essays['generated']\n",
        "\n",
        "train_index, test_index = train_test_split(train_essays.index, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "FsW-bZJVhUXy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(train_essays['preprocessedText'])\n",
        "\n",
        "X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
        "y_train, y_test = y[train_index], y[test_index]\n"
      ],
      "metadata": {
        "id": "lSBF5CjxhlFP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_essays['word2vecVector'] = train_essays['word2vecVector'].apply(lambda x: np.array(x))\n",
        "\n",
        "X_word2vec = np.stack(train_essays['word2vecVector'].values)\n",
        "\n",
        "X_train_word2vec, X_test_word2vec = X_word2vec[train_index], X_word2vec[test_index]\n",
        "\n",
        "print(X_train_word2vec.shape)\n",
        "print(X_test_word2vec.shape)\n",
        "print(X_train_word2vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLjvTwhYjg5c",
        "outputId": "60a640ba-ac53-4ae1-cfc0-1b715ccd14ca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1102, 100)\n",
            "(276, 100)\n",
            "[[ 0.116435   -0.00916288 -0.4983675  ... -0.49612486  0.27139837\n",
            "  -0.06419957]\n",
            " [-0.01954295  0.19030935 -0.33091646 ... -0.25617954  0.13824639\n",
            "   0.04567866]\n",
            " [-0.7795138   0.47796938  0.65176296 ... -0.47629836  0.05644946\n",
            "  -0.0418004 ]\n",
            " ...\n",
            " [-0.01678405  0.02067794 -0.32247236 ... -0.5874986   0.24766058\n",
            "  -0.2081457 ]\n",
            " [-0.89122987  0.4731477   0.7921602  ... -0.40872023 -0.06214165\n",
            "   0.02992062]\n",
            " [-0.02398463  0.32435992 -0.33321595 ... -0.21148597  0.07829262\n",
            "  -0.08271158]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_essays['doc2vecVector'] = train_essays['doc2vecVector'].apply(lambda x: np.array(x))\n",
        "X_doc2vec = np.stack(train_essays['doc2vecVector'].values)\n",
        "X_train_doc2vec, X_test_doc2vec = X_doc2vec[train_index], X_doc2vec[test_index]\n",
        "print(X_train_doc2vec.shape)\n",
        "print(X_test_doc2vec.shape)\n",
        "print(X_train_doc2vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_15evjXjwIk",
        "outputId": "c92085de-50d0-4594-e392-d6cce75ccfd0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1102, 100)\n",
            "(276, 100)\n",
            "[[-0.15162046 -0.3150435  -0.39042804 ... -0.13023089  0.4106068\n",
            "   0.01839693]\n",
            " [-0.41978338 -0.16028115 -0.8079136  ... -0.06477406  0.25259474\n",
            "   0.15097798]\n",
            " [-0.6349936   0.06294966  0.2266118  ... -0.34687087  0.1240015\n",
            "   0.29605973]\n",
            " ...\n",
            " [-0.2682556  -0.23802033 -0.43457854 ... -0.67251056  0.6902878\n",
            "  -0.13823205]\n",
            " [-0.49591488  0.18285985  0.09484906 ... -0.32769707  0.14330235\n",
            "   0.00381349]\n",
            " [-0.28169638 -0.13951762 -0.39002886 ...  0.10226408  0.08423739\n",
            "   0.03543597]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    svm_model = SVC(class_weight='balanced')\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "\n",
        "results_tfidf = evaluate_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results:\", results_doc2vec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeegEozDkbp5",
        "outputId": "09804df1-7c40-4dbc-e36a-a4b425172727"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Word2Vec Results: {'Accuracy': 0.9891304347826086, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Doc2Vec Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    svm_model = SVC(class_weight='balanced')\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "\n",
        "results_tfidf = evaluate_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results:\", results_doc2vec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09804df1-7c40-4dbc-e36a-a4b425172727",
        "id": "fwmYs9TX0uW4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Word2Vec Results: {'Accuracy': 0.9891304347826086, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Doc2Vec Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    svm_model = SVC(class_weight='balanced')\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "\n",
        "results_tfidf = evaluate_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results:\", results_doc2vec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09804df1-7c40-4dbc-e36a-a4b425172727",
        "id": "Ms6E55KI0wp7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Word2Vec Results: {'Accuracy': 0.9891304347826086, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Doc2Vec Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    svm_model = SVC(class_weight='balanced')\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "\n",
        "results_tfidf = evaluate_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results:\", results_doc2vec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09804df1-7c40-4dbc-e36a-a4b425172727",
        "id": "J0yFlmdN0yuk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Word2Vec Results: {'Accuracy': 0.9891304347826086, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Doc2Vec Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN05cpwVWr7kpQYB7PMPQ1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}