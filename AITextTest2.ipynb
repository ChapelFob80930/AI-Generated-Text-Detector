{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChapelFob80930/AI-Generated-Text-Detector/blob/main/AITextTest2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing the Dataset**"
      ],
      "metadata": {
        "id": "KSkesjPZFWta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DivI8E17Ug5",
        "outputId": "e021f19b-e9da-4ea1-8b26-355551a9aec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44868, 5)\n",
            "                                                text  label\n",
            "0  Phones\\n\\nModern humans today are always on th...      0\n",
            "1  This essay will explain if drivers should or s...      0\n",
            "2  Driving while the use of cellular devices\\n\\nT...      0\n",
            "3  Phones & Driving\\n\\nDrivers should not be able...      0\n",
            "4  Cell Phone Operation While Driving\\n\\nThe abil...      0\n",
            "(44868, 2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "ai_generated_dataset = pd.read_csv(\"/content/train_v2_drcat_02.csv\")\n",
        "print(ai_generated_dataset.shape)\n",
        "ai_generated_dataset = ai_generated_dataset.dropna()\n",
        "ai_generated_dataset = ai_generated_dataset.drop(columns=['prompt_name','source','RDizzl3_seven'])\n",
        "print(ai_generated_dataset.head())\n",
        "print(ai_generated_dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gsRlIA2q7kps",
        "outputId": "60ef0954-3d1a-44bd-da7b-65d7dfda1e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_eng to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_rus to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_rus.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_ne_chunker_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package punkt_tab to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt_tab.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package tagsets_json to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets_json.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> Q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# def lemmatize_text(text):\n",
        "#   doc = nlp(\" \".join(text))\n",
        "#   return [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "KtQ0Ruc1qZGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# def _remove_stopwords_(texts):\n",
        "#   return [text for text in texts if text.isalnum() and text not in stop_words]"
      ],
      "metadata": {
        "id": "PWbPW99ZqEQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4HoVvtL8tfB"
      },
      "outputs": [],
      "source": [
        "# from nltk.tokenize import word_tokenize\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# def preprocess_text(series):\n",
        "#     def process(text):\n",
        "#         tokens = word_tokenize(text)\n",
        "#         tokens = _remove_stopwords_(tokens)\n",
        "#         tokens = lemmatize_text(tokens)\n",
        "#         return \" \".join(tokens)\n",
        "#     return series.apply(process)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preprocessing Data**"
      ],
      "metadata": {
        "id": "xOhNVLWSHXBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm.notebook import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\",\"\",text.lower())\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if token.text not in stop_words and token.is_alpha]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def parallel_preprocessing(series):\n",
        "    \"\"\"Preprocess text using spaCy, parallel processing, and tqdm for progress tracking.\"\"\"\n",
        "    tqdm.pandas()\n",
        "    return Parallel(n_jobs=10, backend=\"multiprocessing\")(delayed(preprocess)(text) for text in tqdm(series, desc=\"Preprocessing\"))\n",
        "\n",
        "\n",
        "ai_generated_dataset[\"preprocessedText\"] = parallel_preprocessing(ai_generated_dataset[\"text\"])\n",
        "\n",
        "print(ai_generated_dataset.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "f6a747bcdbe24851b04f9693421905f1",
            "e8c8401f888241f484a266c74ac75818",
            "7b30f226cc08402d8d762a733062ffcc",
            "8e7e99d7097445458313603710be3ecb",
            "9eaab44085234db48a49cd4359558cb6",
            "090ec07adf724293b46e6c8bda5698c3",
            "fc2db5ab1b854a48944e99d25327690d",
            "b595235902d4458ebe4ca4f85b8e1bc8",
            "c261731cb14b4a5cb255d581de1f999d",
            "2924dea43a7c4653875f81ef6bea94dd",
            "1592b7289eb544bb8217489a412f3718"
          ]
        },
        "id": "ht-kx-PVVMDm",
        "outputId": "2bc4707f-2898-4a51-cd84-8ca70e45869f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Preprocessing:   0%|          | 0/44868 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6a747bcdbe24851b04f9693421905f1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_essays[\"preprocessedText\"]=preprocess_text(train_essays['text'])"
      ],
      "metadata": {
        "id": "goB1poMYtSK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ai_human_dataset[\"preprocessedText\"]=preprocess_text(ai_human_dataset['text'])"
      ],
      "metadata": {
        "id": "9lM7gsVmrcue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_essays.head())"
      ],
      "metadata": {
        "id": "oAyccXk8xdTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(ai_human_dataset.head())"
      ],
      "metadata": {
        "id": "3hBGujKSrl7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# sentences = train_essays['preprocessedText'].apply(lambda x: x.split())\n",
        "\n",
        "# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# word_vector = word2vec_model.wv['car']\n",
        "# print(\"Vector for 'car':\", word_vector)\n",
        "\n",
        "# # train_essays['word2vecVector'] = sentences.apply(\n",
        "# #     lambda x: sum([word2vec_model.wv[word] for word in x if word in word2vec_model.wv]) / len(x)\n",
        "# # )\n",
        "\n",
        "# def calculate_average_word_vector(words,model,vector_size=100):\n",
        "#     word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "#     if word_vectors:\n",
        "#         return np.mean(word_vectors, axis=0)\n",
        "#     else:\n",
        "#         return np.zeros(vector_size)\n",
        "\n",
        "# train_essays['word2vecVector'] = train_essays['preprocessedText'].apply(lambda x: calculate_average_word_vector(x.split(),word2vec_model))\n"
      ],
      "metadata": {
        "id": "TIsyFKawzzdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# sentences = ai_human_dataset[\"preprocessedText\"].apply(lambda x: x.split())\n",
        "\n",
        "# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# word_vector = word2vec_model.wv['car']\n",
        "# print(\"Vector for 'car':\", word_vector)\n",
        "\n",
        "# # train_essays['word2vecVector'] = sentences.apply(\n",
        "# #     lambda x: sum([word2vec_model.wv[word] for word in x if word in word2vec_model.wv]) / len(x)\n",
        "# # )\n",
        "\n",
        "# def calculate_average_word_vector(words,model,vector_size=100):\n",
        "#     word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "#     if word_vectors:\n",
        "#         return np.mean(word_vectors, axis=0)\n",
        "#     else:\n",
        "#         return np.zeros(vector_size)\n",
        "\n",
        "# ai_human_dataset['word2vecVector'] = ai_human_dataset[\"preprocessedText\"].apply(lambda x: calculate_average_word_vector(x.split(),word2vec_model))\n"
      ],
      "metadata": {
        "id": "9W9VYPw_z3Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating Word2Vec Embeddings**"
      ],
      "metadata": {
        "id": "GeNwbv6sHxgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = ai_generated_dataset[\"preprocessedText\"].apply(lambda x: x.split())\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=300, window=5, min_count=2, workers=4, sg=1)\n",
        "\n",
        "word_vector = word2vec_model.wv['car']\n",
        "print(\"Vector for 'car':\", word_vector)\n",
        "\n",
        "\n",
        "def calculate_average_word_vector(words,model,vector_size=300):\n",
        "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "def parallel_word2vec(row):\n",
        "    return calculate_average_word_vector(row.split(), word2vec_model)\n",
        "\n",
        "# ai_generated_dataset['word2vecVector'] = ai_generated_dataset[\"preprocessedText\"].apply(lambda x: calculate_average_word_vector(x.split(),word2vec_model))\n",
        "ai_generated_dataset['word2vecVector'] = Parallel(n_jobs=10,backend='multiprocessing')(delayed(parallel_word2vec)(row) for row in tqdm(ai_generated_dataset[\"preprocessedText\"], desc=\"Word2Vec\"))\n",
        "print(ai_generated_dataset.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "oRPabtS3V0Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(train_essays['preprocessedText'])]\n",
        "\n",
        "# doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# train_essays['doc2vecVector'] = train_essays.index.map(lambda x: doc2vec_model.dv[str(x)])\n"
      ],
      "metadata": {
        "id": "zKrP9kmsgqv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(ai_human_dataset[\"preprocessedText\"])]\n",
        "\n",
        "# doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# ai_human_dataset['doc2vecVector'] = ai_human_dataset.index.map(lambda x: doc2vec_model.dv[str(x)])"
      ],
      "metadata": {
        "id": "VDkQdUMS0GRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating Doc2Vec Embeddings**"
      ],
      "metadata": {
        "id": "qotyAPoIH8oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "\n",
        "def prepare_tagged_documents(series):\n",
        "    \"\"\"Prepare tagged documents for Doc2Vec.\"\"\"\n",
        "    return [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(series)]\n",
        "\n",
        "\n",
        "tqdm.pandas(desc=\"Preparing Tagged Documents\")\n",
        "tagged_data = prepare_tagged_documents(tqdm(ai_generated_dataset[\"preprocessedText\"], desc=\"Tagging Data\"))\n",
        "\n",
        "\n",
        "doc2vec_model = Doc2Vec(tagged_data, vector_size=300, window=5, min_count=2, workers=4, dm=1,epochs=20)\n",
        "\n",
        "# Function to generate Doc2Vec vectors\n",
        "def generate_doc2vec_vector(index):\n",
        "    \"\"\"Generate a Doc2Vec vector for a given index.\"\"\"\n",
        "    return doc2vec_model.dv[str(index)]\n",
        "\n",
        "# Use tqdm and joblib to parallelize vector generation\n",
        "def generate_doc2vec_vectors_parallel(dataframe):\n",
        "    \"\"\"Generate Doc2Vec vectors in parallel with progress tracking.\"\"\"\n",
        "    indices = dataframe.index\n",
        "    return Parallel(n_jobs=10, backend=\"multiprocessing\")(delayed(generate_doc2vec_vector)(index) for index in tqdm(indices, desc=\"Generating Doc2Vec Vectors\"))\n",
        "\n",
        "ai_generated_dataset['doc2vecVector'] = generate_doc2vec_vectors_parallel(ai_generated_dataset)\n",
        "\n",
        "\n",
        "print(ai_generated_dataset.head())\n"
      ],
      "metadata": {
        "id": "qvgs8xGcnjU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_essays.head())"
      ],
      "metadata": {
        "id": "JeU2USKZg8LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# y = train_essays['generated']\n",
        "\n",
        "# train_index, test_index = train_test_split(train_essays.index, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "FsW-bZJVhUXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Splitting the Test and Train indexes and Separating the Labels from the Data**"
      ],
      "metadata": {
        "id": "6fsEKQraIHwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = ai_generated_dataset['label']\n",
        "\n",
        "train_index, test_index = train_test_split(ai_generated_dataset.index, test_size=0.3, random_state=42, stratify = y)\n"
      ],
      "metadata": {
        "id": "_i0ssR6L0dp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_index.shape)\n",
        "print(test_index.shape)"
      ],
      "metadata": {
        "id": "Kp95xJPV_XW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# y = ai_human_dataset['generated']\n",
        "\n",
        "# train_index, test_index = train_test_split(ai_human_dataset.index, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "AKsDAiNCoXfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "# X_tfidf = tfidf_vectorizer.fit_transform(train_essays['preprocessedText'])\n",
        "\n",
        "# X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
        "# y_train, y_test = y[train_index], y[test_index]\n"
      ],
      "metadata": {
        "id": "lSBF5CjxhlFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating TF-IDF Embeddings**"
      ],
      "metadata": {
        "id": "htJAfPMrIl5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000,ngram_range=(1,2),max_df=0.95,min_df=2)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(ai_generated_dataset[\"preprocessedText\"])\n",
        "\n",
        "X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
        "y_train, y_test = y[train_index], y[test_index]\n"
      ],
      "metadata": {
        "id": "PH4Lysdq0osa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_tfidf.shape)"
      ],
      "metadata": {
        "id": "DAvQGcNgs_9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_tfidf_balanced, y_train_tfidf_balanced = smote.fit_resample(X_train_tfidf, y_train)\n",
        "print(\"TF-IDF Before SMOTE: \",dict(zip(*np.unique(y_train, return_counts=True))))\n",
        "print(\"TF-IDF After SMOTE: \",dict(zip(*np.unique(y_train_tfidf_balanced, return_counts=True))))"
      ],
      "metadata": {
        "id": "nrXKKIi9ivQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Splitting Word2Vec Embeddings into Train and Test Datasets**"
      ],
      "metadata": {
        "id": "7PRuuvL9IupS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_generated_dataset['word2vecVector'] = ai_generated_dataset['word2vecVector'].apply(lambda x: np.array(x))\n",
        "\n",
        "X_word2vec = np.stack(ai_generated_dataset['word2vecVector'].values)\n",
        "\n",
        "X_train_word2vec, X_test_word2vec = X_word2vec[train_index], X_word2vec[test_index]\n",
        "\n",
        "print(X_train_word2vec.shape)\n",
        "print(X_test_word2vec.shape)\n",
        "print(X_train_word2vec)"
      ],
      "metadata": {
        "id": "JLjvTwhYjg5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_word2vec_balanced, y_train_word2vec_balanced = smote.fit_resample(X_train_word2vec, y_train)\n",
        "print(\"Word2Vec Before SMOTE: \",dict(zip(*np.unique(y_train, return_counts=True))))\n",
        "print(\"Word2Vec After SMOTE: \",dict(zip(*np.unique(y_train_word2vec_balanced, return_counts=True))))"
      ],
      "metadata": {
        "id": "P6OlXbBLjMUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Splitting Doc2Vec Embeddings into Train and Test Datasets**"
      ],
      "metadata": {
        "id": "o5c0RNmGJA3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_generated_dataset['doc2vecVector'] = ai_generated_dataset['doc2vecVector'].apply(lambda x: np.array(x))\n",
        "X_doc2vec = np.stack(ai_generated_dataset['doc2vecVector'].values)\n",
        "X_train_doc2vec, X_test_doc2vec = X_doc2vec[train_index], X_doc2vec[test_index]\n",
        "print(X_train_doc2vec.shape)\n",
        "print(X_test_doc2vec.shape)\n",
        "print(X_train_doc2vec)"
      ],
      "metadata": {
        "id": "R_15evjXjwIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_doc2vec_balanced, y_train_doc2vec_balanced = smote.fit_resample(X_train_doc2vec, y_train)\n",
        "print(\"Doc2Vec Before SMOTE: \",dict(zip(*np.unique(y_train, return_counts=True))))\n",
        "print(\"Doc2Vec After SMOTE: \",dict(zip(*np.unique(y_train_doc2vec_balanced, return_counts=True))))"
      ],
      "metadata": {
        "id": "0paJLA_BjVrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing SVM on Different Types of Embeddings and Comparing Results**"
      ],
      "metadata": {
        "id": "HP58o58fJNgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing Decision Tree on Different Types of Embeddings and Comparing Results**"
      ],
      "metadata": {
        "id": "o3wIU2XXJqHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing Random Forest on Different Types of Embeddings and Comparing Results**"
      ],
      "metadata": {
        "id": "rtFEfGBJKKsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing KNN on Different Types of Embeddings and Comparing Results**"
      ],
      "metadata": {
        "id": "HIxP3NsGKUo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model_with_visualizations(model, X_train, X_test, y_train, y_test, model_name, embedding_name):\n",
        "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    print(f\"Performing 10-Fold Cross-Validation for {model_name} with {embedding_name}...\")\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
        "    print(f\"{model_name} with {embedding_name} - Cross-validation Scores: {cv_scores}\")\n",
        "    print(f\"{model_name} with {embedding_name} - Mean CV Accuracy: {cv_scores.mean():.4f}\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix: {model_name} with {embedding_name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"{model_name} with {embedding_name} - Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(range(len(y_test)), y_test, alpha=0.6, label='Actual')\n",
        "    plt.scatter(range(len(y_pred)), y_pred, alpha=0.6, label='Predicted', marker='x')\n",
        "    plt.title(f'Scatter Plot: {model_name} with {embedding_name}')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Labels')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ryeiUpHu0h08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "8MRNQuOY0ko7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# svm_param_grid = {\n",
        "#     'C': [0.1, 1, 10],\n",
        "#     'kernel': ['linear', 'rbf'],\n",
        "#     'gamma': [0.01, 0.1, 1]\n",
        "# }\n",
        "\n",
        "# dt_param_grid = {\n",
        "#     'max_depth': [None, 10, 20],\n",
        "#     'min_samples_split': [2, 5, 10]\n",
        "# }\n",
        "\n",
        "# rf_param_grid = {\n",
        "#     'n_estimators': [50, 100, 200],\n",
        "#     'max_depth': [None, 10, 20],\n",
        "#     'min_samples_split': [2, 5, 10]\n",
        "# }\n",
        "\n",
        "# knn_param_grid = {\n",
        "#     'n_neighbors': [3, 5, 7],\n",
        "#     'weights': ['uniform', 'distance']\n",
        "# }"
      ],
      "metadata": {
        "id": "tF8TWpV79LRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_generated_dataset['label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "9tZ8Jfg41osr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model_tfidf = SVC(class_weight='balanced',random_state=42,kernel='rbf',gamma='scale',C=1)\n",
        "evaluate_model_with_visualizations(svm_model_tfidf, X_train_tfidf_balanced, X_test_tfidf, y_train_tfidf_balanced, y_test, \"SVM\", \"TF-IDF\")"
      ],
      "metadata": {
        "id": "9_QjzkU80yRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model_word2vec = SVC(class_weight='balanced',random_state=42, C=1, kernel='rbf', gamma='scale')\n",
        "evaluate_model_with_visualizations(svm_model_word2vec, X_train_word2vec_balanced, X_test_word2vec, y_train_word2vec_balanced, y_test, \"SVM\", \"Word2Vec\")"
      ],
      "metadata": {
        "id": "WH3d9dpz2w6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model_doc2vec = SVC(class_weight='balanced',random_state=42, C=1, kernel='rbf', gamma='scale')\n",
        "evaluate_model_with_visualizations(svm_model_doc2vec, X_train_doc2vec_balanced, X_test_doc2vec, y_train_doc2vec_balanced, y_test, \"SVM\", \"Doc2Vec\")"
      ],
      "metadata": {
        "id": "oKeDcxgi28Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree_model_tfidf = DecisionTreeClassifier(class_weight='balanced',random_state=42, criterion='gini',max_depth=None,min_samples_split=5)\n",
        "evaluate_model_with_visualizations(decision_tree_model_tfidf, X_train_tfidf_balanced, X_test_tfidf, y_train_tfidf_balanced, y_test, \"Decision Tree\", \"TF-IDF\")"
      ],
      "metadata": {
        "id": "Hyn48k7L2rBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree_model_word2vec = DecisionTreeClassifier(class_weight='balanced',random_state=42, criterion='gini', max_depth=None, min_samples_split=5)\n",
        "evaluate_model_with_visualizations(decision_tree_model_word2vec, X_train_word2vec_balanced, X_test_word2vec, y_train_word2vec_balanced, y_test, \"Decision Tree\", \"Word2Vec\")"
      ],
      "metadata": {
        "id": "WxBCQArR2zPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree_model_doc2vec = DecisionTreeClassifier(class_weight='balanced',random_state=42, max_depth=None, min_samples_split=5)\n",
        "evaluate_model_with_visualizations(decision_tree_model_doc2vec, X_train_doc2vec_balanced, X_test_doc2vec, y_train_doc2vec_balanced, y_test, \"Decision Tree\", \"Doc2Vec\")"
      ],
      "metadata": {
        "id": "CZiNGuyq2-E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_model_tfidf = RandomForestClassifier(class_weight='balanced', random_state=42,n_estimators=200,max_depth=None,min_samples_split=5)\n",
        "evaluate_model_with_visualizations(random_forest_model_tfidf, X_train_tfidf_balanced, X_test_tfidf, y_train_tfidf_balanced, y_test, \"Random Forest\", \"TF-IDF\")"
      ],
      "metadata": {
        "id": "rXxqtcf_2tGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_model_word2vec = RandomForestClassifier(class_weight='balanced',random_state=42, n_estimators=200, max_depth=None, min_samples_split=5)\n",
        "evaluate_model_with_visualizations(random_forest_model_word2vec, X_train_word2vec_balanced, X_test_word2vec, y_train_word2vec_balanced, y_test, \"Random Forest\", \"Word2Vec\")"
      ],
      "metadata": {
        "id": "2cU9Xmg_23SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_model_doc2vec = RandomForestClassifier(class_weight='balanced',random_state=42, n_estimators=200, max_depth=None, min_samples_split=5)\n",
        "evaluate_model_with_visualizations(random_forest_model_doc2vec, X_train_doc2vec_balanced, X_test_doc2vec, y_train_doc2vec_balanced, y_test, \"Random Forest\", \"Doc2Vec\")"
      ],
      "metadata": {
        "id": "2qbvlJU_2_de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model_tfidf = KNeighborsClassifier(n_neighbors=3,weights='distance',metric='minkowski')\n",
        "evaluate_model_with_visualizations(knn_model_tfidf, X_train_tfidf_balanced, X_test_tfidf, y_train_tfidf_balanced, y_test, \"KNN\", \"TF-IDF\")"
      ],
      "metadata": {
        "id": "4cPGb0Jt2ufd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model_word2vec = KNeighborsClassifier(n_neighbors=4,weights='distance',metric='minkowski')\n",
        "evaluate_model_with_visualizations(knn_model_word2vec, X_train_word2vec_balanced, X_test_word2vec, y_train_word2vec_balanced, y_test, \"KNN\", \"Word2Vec\")"
      ],
      "metadata": {
        "id": "3fzO_rbT26Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model_doc2vec = KNeighborsClassifier(n_neighbors=2,weights='distance',metric='minkowski')\n",
        "evaluate_model_with_visualizations(knn_model_doc2vec, X_train_doc2vec_balanced, X_test_doc2vec, y_train_doc2vec_balanced, y_test, \"KNN\", \"Doc2Vec\")"
      ],
      "metadata": {
        "id": "dt8qfCmS3A10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing the best embedding technique for each model and using it on custom data**\n",
        "\n",
        "\n",
        "#1.   SVM :- TF-IDF\n",
        "#2.   Decision Tree :- Word2Vec\n",
        "#3.   Random Forest :- TF-IDF\n",
        "#4.   KNN :- Word2Vec  \n",
        "\n"
      ],
      "metadata": {
        "id": "q7dnSsjsKgdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# svm_model = SVC(class_weight='balanced',random_state=42)\n",
        "# svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "# decision_tree_model.fit(X_train_word2vec, y_train)\n",
        "\n",
        "# random_forest_model = RandomForestClassifier(random_state=42)\n",
        "# random_forest_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# knn_model = KNeighborsClassifier(n_neighbors=3)\n",
        "# knn_model.fit(X_train_word2vec, y_train)"
      ],
      "metadata": {
        "id": "Q5m3E8tjqZ3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_data = input()\n",
        "#custom_data = \"Good Morning Everyone, I am your teacher for today\"\n",
        "preprocessed_custom_data = preprocess(custom_data)\n",
        "print(preprocessed_custom_data)\n",
        "#for word in preprocessed_custom_data.split():\n",
        "#    if word not in word2vec_model.wv:\n",
        "#        print(f\"Missing word: {word}\")\n",
        "\n",
        "custom_data_tfidf = tfidf_vectorizer.transform([preprocessed_custom_data])\n",
        "\n",
        "#def update_word2vec_with_custom_data(preprocessed_data, word2vec_model):\n",
        "#    \"\"\"Updating Word2Vec model with custom data if new words are found.\"\"\"\n",
        "#    new_sentences = [preprocessed_data.split()]\n",
        "\n",
        "#    word2vec_model.build_vocab(new_sentences, update=True)\n",
        "#    word2vec_model.train(new_sentences, total_examples=len(new_sentences), epochs=5)\n",
        "\n",
        "\n",
        "#update_word2vec_with_custom_data(preprocessed_custom_data, word2vec_model)\n",
        "\n",
        "#custom_data_word2vec = [calculate_average_word_vector(preprocessed_custom_data.split(), word2vec_model, word2vec_model.vector_size)]\n",
        "custom_data_word2vec = np.array([calculate_average_word_vector(preprocessed_custom_data.split(), word2vec_model, word2vec_model.vector_size)]).reshape(1, -1)\n",
        "\n",
        "print(custom_data_tfidf.shape)\n",
        "print(custom_data_word2vec.shape)\n",
        "\n",
        "\n",
        "prediction_svm = svm_model_tfidf.predict(custom_data_tfidf)\n",
        "print(\"Prediction using SVM using TF-IDF:\", prediction_svm)\n",
        "prediction_svm = svm_model_word2vec.predict(custom_data_word2vec)\n",
        "print(\"Prediction using SVM using Word2Vec:\", prediction_svm)\n",
        "\n",
        "prediction_decision_tree = decision_tree_model_tfidf.predict(custom_data_tfidf)\n",
        "print(\"Prediction using Decision Tree using TF-IDF:\", prediction_decision_tree)\n",
        "prediction_decision_tree = decision_tree_model_word2vec.predict(custom_data_word2vec)\n",
        "print(\"Prediction using Decision Tree using Word2Vec:\", prediction_decision_tree)\n",
        "\n",
        "prediction_random_forest = random_forest_model_tfidf.predict(custom_data_tfidf)\n",
        "print(\"Prediction using Random Forest using TF-IDF:\", prediction_random_forest)\n",
        "prediction_random_forest = random_forest_model_word2vec.predict(custom_data_word2vec)\n",
        "print(\"Prediction using Random Forest using Word2Vec:\", prediction_random_forest)\n",
        "\n",
        "prediction_knn = knn_model_tfidf.predict(custom_data_tfidf)\n",
        "print(\"Prediction using KNN using TF-IDF:\", prediction_knn)\n",
        "prediction_knn = knn_model_word2vec.predict(custom_data_word2vec)\n",
        "print(\"Prediction using KNN using Word2Vec:\", prediction_knn)"
      ],
      "metadata": {
        "id": "orWKAnHwDBFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4YTiYUUWEAsj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODHLVyIQv5Ni1/FWKKCnCN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6a747bcdbe24851b04f9693421905f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8c8401f888241f484a266c74ac75818",
              "IPY_MODEL_7b30f226cc08402d8d762a733062ffcc",
              "IPY_MODEL_8e7e99d7097445458313603710be3ecb"
            ],
            "layout": "IPY_MODEL_9eaab44085234db48a49cd4359558cb6"
          }
        },
        "e8c8401f888241f484a266c74ac75818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_090ec07adf724293b46e6c8bda5698c3",
            "placeholder": "​",
            "style": "IPY_MODEL_fc2db5ab1b854a48944e99d25327690d",
            "value": "Preprocessing:  16%"
          }
        },
        "7b30f226cc08402d8d762a733062ffcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b595235902d4458ebe4ca4f85b8e1bc8",
            "max": 44868,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c261731cb14b4a5cb255d581de1f999d",
            "value": 7190
          }
        },
        "8e7e99d7097445458313603710be3ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2924dea43a7c4653875f81ef6bea94dd",
            "placeholder": "​",
            "style": "IPY_MODEL_1592b7289eb544bb8217489a412f3718",
            "value": " 7180/44868 [14:40&lt;1:35:15,  6.59it/s]"
          }
        },
        "9eaab44085234db48a49cd4359558cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090ec07adf724293b46e6c8bda5698c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc2db5ab1b854a48944e99d25327690d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b595235902d4458ebe4ca4f85b8e1bc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c261731cb14b4a5cb255d581de1f999d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2924dea43a7c4653875f81ef6bea94dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1592b7289eb544bb8217489a412f3718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}