{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChapelFob80930/AI-Generated-Text-Detector/blob/main/AITextTest2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DivI8E17Ug5",
        "outputId": "c4d59f6e-b847-4d84-ba19-2f8f07cfc583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   prompt_id                       prompt_name  \\\n",
            "0          0                   Car-free cities   \n",
            "1          1  Does the electoral college work?   \n",
            "\n",
            "                                        instructions  \\\n",
            "0  Write an explanatory essay to inform fellow ci...   \n",
            "1  Write a letter to your state senator in which ...   \n",
            "\n",
            "                                         source_text  \n",
            "0  # In German Suburb, Life Goes On Without Cars ...  \n",
            "1  # What Is the Electoral College? by the Office...  \n",
            "\n",
            "\n",
            "         id  prompt_id                                               text  \\\n",
            "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
            "1  005db917          0  Transportation is a large necessity in most co...   \n",
            "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
            "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
            "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
            "\n",
            "   generated  \n",
            "0          0  \n",
            "1          0  \n",
            "2          0  \n",
            "3          0  \n",
            "4          0  \n",
            "\n",
            "\n",
            "         id  prompt_id          text\n",
            "0  0000aaaa          2  Aaa bbb ccc.\n",
            "1  1111bbbb          3  Bbb ccc ddd.\n",
            "2  2222cccc          4  CCC ddd eee.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train_prompts = pd.read_csv(\"https://raw.githubusercontent.com/ChapelFob80930/AI-Generated-Text-Detector/refs/heads/main/train_prompts.csv\")\n",
        "print(train_prompts.head())\n",
        "print(\"\\n\")\n",
        "train_essays = pd.read_csv(\"https://raw.githubusercontent.com/ChapelFob80930/AI-Generated-Text-Detector/refs/heads/main/train_essays.csv\")\n",
        "print(train_essays.head())\n",
        "print(\"\\n\")\n",
        "test_essays = pd.read_csv(\"https://raw.githubusercontent.com/ChapelFob80930/AI-Generated-Text-Detector/refs/heads/main/test_essays.csv\")\n",
        "print(test_essays.head())\n",
        "# ai_human_dataset = pd.read_csv(\"/content/AI_Human.csv\")\n",
        "# print(ai_human_dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gsRlIA2q7kps",
        "outputId": "567302df-9ece-480d-a8d8-f3b542af8827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> all\n",
            "Command 'all' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_eng to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_rus to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_rus.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_ne_chunker_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package punkt_tab to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt_tab.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package tagsets_json to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets_json.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  doc = nlp(\" \".join(text))\n",
        "  return [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "KtQ0Ruc1qZGW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def _remove_stopwords_(texts):\n",
        "  return [text for text in texts if text.isalnum() and text not in stop_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWbPW99ZqEQi",
        "outputId": "e81ccf32-e741-4755-82a6-e34c8cbc64ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4HoVvtL8tfB",
        "outputId": "16c6c298-397a-477a-bfc6-4d4ee758b2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(series):\n",
        "    def process(text):\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = _remove_stopwords_(tokens)\n",
        "        tokens = lemmatize_text(tokens)\n",
        "        return \" \".join(tokens)\n",
        "    return series.apply(process)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_essays[\"preprocessedText\"]=preprocess_text(train_essays['text'])"
      ],
      "metadata": {
        "id": "goB1poMYtSK_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_essays.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAyccXk8xdTm",
        "outputId": "8e96f501-f95a-4faf-9255-aba07e8ed829"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id  prompt_id                                               text  \\\n",
            "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
            "1  005db917          0  Transportation is a large necessity in most co...   \n",
            "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
            "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
            "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
            "\n",
            "   generated                                   preprocessedText  \n",
            "0          0  car Cars around since become famous 1900s Henr...  \n",
            "1          0  transportation large necessity country worldwi...  \n",
            "2          0  America love affair vehicle seem cool say Elis...  \n",
            "3          0  how often ride car do drive one motor vehicle ...  \n",
            "4          0  car wonderful thing they perhaps one world gre...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = train_essays['preprocessedText'].apply(lambda x: x.split())\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "word_vector = word2vec_model.wv['car']\n",
        "print(\"Vector for 'car':\", word_vector)\n",
        "\n",
        "# train_essays['word2vecVector'] = sentences.apply(\n",
        "#     lambda x: sum([word2vec_model.wv[word] for word in x if word in word2vec_model.wv]) / len(x)\n",
        "# )\n",
        "\n",
        "def calculate_average_word_vector(words,model,vector_size=100):\n",
        "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "train_essays['word2vecVector'] = train_essays['preprocessedText'].apply(lambda x: calculate_average_word_vector(x.split(),word2vec_model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIsyFKawzzdF",
        "outputId": "b715af81-123d-4df7-d571-4e670f8213a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'car': [ 1.2111949  -0.30556297 -0.7253577   1.6476825  -0.05973133 -2.5271766\n",
            "  2.3698192   0.08536696 -1.6918591   1.0612195   0.36911753  0.937336\n",
            "  0.02407086  1.6470591  -0.9913625  -0.76151234  0.10906904 -0.95410055\n",
            " -1.5863189  -3.0185814   2.0043833   0.4172694  -0.10092472 -0.33763146\n",
            " -0.3924448  -1.1022891  -1.208857   -0.21761449 -1.909737   -0.72541547\n",
            " -1.1375293   0.80547315  0.58267814  0.17105478  0.4518194  -0.2760079\n",
            "  0.12584671 -1.1856534   1.0627922  -2.2725573  -0.41128555  0.24149497\n",
            " -1.6320343   0.1893235  -0.10478523 -0.3555176  -0.5094247   1.0395855\n",
            "  2.5929909   1.5383127  -0.1976783   1.1053098   0.4914338   0.5094458\n",
            "  1.3409163  -0.9554275   0.45697558 -0.18407106 -2.277477    0.38295817\n",
            "  0.50247586 -0.0343245  -0.05879904 -1.6111666  -0.8870315   1.5619276\n",
            " -0.23527811  0.5536205  -0.6408502  -0.21213892  1.0205584   0.10475906\n",
            "  0.3971731  -2.3980396   1.4803641  -0.5004348   2.3631022   0.2105331\n",
            " -1.5404352  -0.70773375  0.1468584   0.42568913 -0.8826514   1.3289313\n",
            " -0.01301712 -0.9908688   1.4364457   0.45884657  0.07058564  1.0798415\n",
            " -0.74475646  0.30378467  0.64145327  0.2954928   1.2960643  -0.23682156\n",
            "  0.59858173  0.27972972  0.7398664   0.5200944 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(train_essays['preprocessedText'])]\n",
        "\n",
        "doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "train_essays['doc2vecVector'] = train_essays.index.map(lambda x: doc2vec_model.dv[str(x)])\n"
      ],
      "metadata": {
        "id": "zKrP9kmsgqv_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_essays.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeU2USKZg8LN",
        "outputId": "b96ec331-7f73-43a4-cbe0-ec63565fd51f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id  prompt_id                                               text  \\\n",
            "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
            "1  005db917          0  Transportation is a large necessity in most co...   \n",
            "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
            "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
            "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
            "\n",
            "   generated                                   preprocessedText  \\\n",
            "0          0  car Cars around since become famous 1900s Henr...   \n",
            "1          0  transportation large necessity country worldwi...   \n",
            "2          0  America love affair vehicle seem cool say Elis...   \n",
            "3          0  how often ride car do drive one motor vehicle ...   \n",
            "4          0  car wonderful thing they perhaps one world gre...   \n",
            "\n",
            "                                      word2vecVector  \\\n",
            "0  [-0.024536112, 0.050590437, -0.5430817, -0.255...   \n",
            "1  [0.07706183, 0.08590059, -0.4557113, -0.173994...   \n",
            "2  [-0.10698871, 0.13882993, -0.32883853, 0.09729...   \n",
            "3  [-0.06126656, 0.12775174, -0.433705, -0.081154...   \n",
            "4  [-0.11286109, 0.1891967, -0.40161952, 0.071160...   \n",
            "\n",
            "                                       doc2vecVector  \n",
            "0  [-0.1421645, -0.16111003, -0.3313632, -0.57383...  \n",
            "1  [-0.16477554, 0.13065921, -0.21027794, 0.05722...  \n",
            "2  [-0.27417967, 0.004892268, -0.20935844, -0.158...  \n",
            "3  [-0.17198515, 0.28151485, -0.32494748, -0.1617...  \n",
            "4  [-0.3245751, 0.08745761, -0.58919126, -0.12171...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = train_essays['generated']\n",
        "\n",
        "train_index, test_index = train_test_split(train_essays.index, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "FsW-bZJVhUXy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(train_essays['preprocessedText'])\n",
        "\n",
        "X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
        "y_train, y_test = y[train_index], y[test_index]\n"
      ],
      "metadata": {
        "id": "lSBF5CjxhlFP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_essays['word2vecVector'] = train_essays['word2vecVector'].apply(lambda x: np.array(x))\n",
        "\n",
        "X_word2vec = np.stack(train_essays['word2vecVector'].values)\n",
        "\n",
        "X_train_word2vec, X_test_word2vec = X_word2vec[train_index], X_word2vec[test_index]\n",
        "\n",
        "print(X_train_word2vec.shape)\n",
        "print(X_test_word2vec.shape)\n",
        "print(X_train_word2vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLjvTwhYjg5c",
        "outputId": "d00e8e16-7565-4a93-d5ae-b22e2d0d5932"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1102, 100)\n",
            "(276, 100)\n",
            "[[ 0.0480455   0.00245276 -0.51658636 ... -0.5090301   0.31441182\n",
            "  -0.10226092]\n",
            " [-0.07436413  0.2377062  -0.37200594 ... -0.25823748  0.19648382\n",
            "   0.1243243 ]\n",
            " [-0.68955594  0.561735    0.5310993  ... -0.5083269  -0.01480051\n",
            "   0.17156057]\n",
            " ...\n",
            " [-0.06672735  0.03907606 -0.3371643  ... -0.6025266   0.28306353\n",
            "  -0.20842443]\n",
            " [-0.86480206  0.5640187   0.64005625 ... -0.43267986 -0.11761905\n",
            "   0.25580472]\n",
            " [-0.0423575   0.36134255 -0.38192007 ... -0.20374025  0.13378517\n",
            "  -0.06544389]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_essays['doc2vecVector'] = train_essays['doc2vecVector'].apply(lambda x: np.array(x))\n",
        "X_doc2vec = np.stack(train_essays['doc2vecVector'].values)\n",
        "X_train_doc2vec, X_test_doc2vec = X_doc2vec[train_index], X_doc2vec[test_index]\n",
        "print(X_train_doc2vec.shape)\n",
        "print(X_test_doc2vec.shape)\n",
        "print(X_train_doc2vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_15evjXjwIk",
        "outputId": "c2573541-f00a-432f-c60d-70b6bdd0683d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1102, 100)\n",
            "(276, 100)\n",
            "[[ 0.07147887 -0.32900298 -0.23523858 ... -0.14498188  0.45644414\n",
            "   0.13322556]\n",
            " [-0.28677464 -0.09217716 -0.73470193 ...  0.05696475  0.27619284\n",
            "   0.3492302 ]\n",
            " [-0.46581587  0.05295126  0.2343993  ... -0.41684785  0.09282099\n",
            "   0.14894862]\n",
            " ...\n",
            " [-0.04361692 -0.14471887 -0.4125112  ... -0.51393485  0.67461795\n",
            "   0.04859068]\n",
            " [-0.4447485   0.18331645  0.11205757 ... -0.43579936  0.04945527\n",
            "   0.16391483]\n",
            " [-0.02295278  0.02559162 -0.3291557  ...  0.00198629 -0.00682148\n",
            "  -0.03970782]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def evaluate_svm_model(X_train, X_test, y_train, y_test):\n",
        "    svm_model = SVC(class_weight='balanced')\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "\n",
        "results_tfidf = evaluate_svm_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results for SVM:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_svm_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results for SVM:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_svm_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results for SVM:\", results_doc2vec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeegEozDkbp5",
        "outputId": "8a63b569-a236-48b1-de9b-54025b4b8c26"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Word2Vec Results: {'Accuracy': 0.9891304347826086, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Doc2Vec Results: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "def evaluate_decisionTree_model(X_train, X_test, y_train, y_test):\n",
        "    dt_model = DecisionTreeClassifier(random_state=42)\n",
        "    dt_model.fit(X_train, y_train)\n",
        "    y_pred = dt_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "results_tfidf = evaluate_decisionTree_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results for Decision Tree:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_decisionTree_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results for Decision Tree:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_decisionTree_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results for Decision Tree:\", results_doc2vec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2f7183-4ffd-48a5-8573-1fc7bb4ce704",
        "id": "fwmYs9TX0uW4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Results for Decision Tree: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Word2Vec Results for Decision Tree: {'Accuracy': 0.9927536231884058, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Doc2Vec Results for Decision Tree: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "def evaluate_randomForest_model(X_train, X_test, y_train, y_test):\n",
        "    rf_model = RandomForestClassifier(random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "results_tfidf = evaluate_randomForest_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results for Random Forest:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_randomForest_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results for Random Forest:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_randomForest_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results for Random Forest:\", results_doc2vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c8bdde8-aeac-419f-b62e-bc72f62dc4df",
        "id": "Ms6E55KI0wp7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Results for Random Forest: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Results for Random Forest: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Doc2Vec Results for Random Forest: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "def evaluate_knn_model(X_train, X_test, y_train, y_test):\n",
        "    knn_model = KNeighborsClassifier()\n",
        "    knn_model.fit(X_train, y_train)\n",
        "    y_pred = knn_model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred,),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "results_tfidf = evaluate_knn_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "print(\"TF-IDF Results for KNN:\", results_tfidf)\n",
        "\n",
        "\n",
        "results_word2vec = evaluate_knn_model(X_train_word2vec, X_test_word2vec, y_train, y_test)\n",
        "print(\"Word2Vec Results for KNN:\", results_word2vec)\n",
        "\n",
        "\n",
        "results_doc2vec = evaluate_knn_model(X_train_doc2vec, X_test_doc2vec, y_train, y_test)\n",
        "print(\"Doc2Vec Results for KNN:\", results_doc2vec)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c19e916-9ab1-420e-e0e8-3969e38161a3",
        "id": "J0yFlmdN0yuk"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Results for KNN: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Word2Vec Results for KNN: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n",
            "Doc2Vec Results for KNN: {'Accuracy': 0.9963768115942029, 'Precision': 0.0, 'Recall': 0.0, 'F1-Score': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9jytG0j3HNc44ZZ28o7c2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}